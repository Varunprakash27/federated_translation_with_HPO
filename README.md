ğŸ§  Federated Translation with Hyperparameter Optimization (HPO)

This project demonstrates a federated learning pipeline for machine translation, enhanced with hyperparameter optimization (HPO) using Optuna.

------------------------------------------------------------

ğŸš€ Features:

- ğŸŒ Federated Learning using Flower to train models across multiple devices without sharing raw data  
- ğŸ¯ Hyperparameter Optimization with Optuna for tuning learning rate, batch size, and number of epochs  
- ğŸ“š Transformer-based translation using MarianMT from HuggingFace  
- ğŸ§ª Logs and stores results from HPO trials for reproducibility and analysis  

------------------------------------------------------------

ğŸ“‚ Project Structure:

federated_translation_with_HPO/  
â”œâ”€â”€ .gitignore              - Ignores folders like venv, pycache, etc.  
â”œâ”€â”€ fl_client.py            - Client-side federated training logic  
â”œâ”€â”€ fl_server.py            - Server to coordinate federated rounds  
â”œâ”€â”€ hpo_optuna.py           - Optuna setup and search strategy  
â”œâ”€â”€ translate.py            - Translation and inference script  
â”œâ”€â”€ utils.py                - Utility functions  
â”œâ”€â”€ requirements.txt        - Required Python packages  
â””â”€â”€ data/                   - Training and testing data  

------------------------------------------------------------

âš™ï¸ Installation:

1. Set up a Python virtual environment  
2. Install the dependencies using:  
   pip install -r requirements.txt

------------------------------------------------------------

â–¶ï¸ How to Run:

1. Start the federated server  
   python fl_server.py


2. Open two separate terminals for clients:
   In Terminal 1:
   python fl_client.py

   In Terminal 2:
   python fl_client.py

   This simulates two clients participating in the federated learning process.

3. To perform hyperparameter optimization  
   python hpo_optuna.py

4. For translation inference  
   python translate.py

------------------------------------------------------------

ğŸ›  Technologies Used:

- Python  
- PyTorch  
- HuggingFace Transformers  
- Flower (FLWR)  
- Optuna  

------------------------------------------------------------

ğŸš« Git Ignore Info:

These files and folders are excluded from version control:  
- venv/  
- __pycache__/  
- *.pt (model files)  
- hpo_results/

------------------------------------------------------------

ğŸ¤ Contributing:

Contributions are welcome! Feel free to open an issue or submit a pull request with improvements or bug fixes.

------------------------------------------------------------

ğŸ“„ License:

This project is open-source and available under the MIT License.
